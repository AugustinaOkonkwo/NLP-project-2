{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761738d0",
   "metadata": {},
   "source": [
    "## Read in files using read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc671477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the files using pandas read_csv\n",
    "df1 = pd.read_csv('Renaissance23July-29July(02_50_07am_to_05_01_07am).csv')\n",
    "df2 = pd.read_csv('Renaissance29July-01August(05_01_07am_to_22_04_49pm).csv')\n",
    "df3 = pd.read_csv('Renaissance1August-3August(22_05_07pm_to_00_07_12pm).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c83300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes as one\n",
    "tweet_df = pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698e213",
   "metadata": {},
   "source": [
    "###  Data Assessment\n",
    "Data was assessed here for data quality issues such as missing data, incorrect datatypes, duplicates e.t.c. and Data Tidiness issues.\n",
    "While assessing for duplicates, tweet id is considered as the Primary key/ Unique identifier for all the dataframe\n",
    "\n",
    "Column Descriptions\n",
    "\n",
    "id - Unique id for each tweet\n",
    "username - The twitter username of the tweeter\n",
    "time_of_tweet - The time the tweet was tweeted\n",
    "tweet - The content of the tweet\n",
    "location - Location of Tweeter\n",
    "retweets - The number of times the tweet has been retweeted\n",
    "likes - The number of times the tweet has been liked\n",
    "followers - The number of followers of the Tweeter\n",
    "following - The number of followings of the Tweeter\n",
    "verified - Whether the Tweeter is verified or not? True/False\n",
    "tweet_source - The Source of Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of rows and columns\n",
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first five rows of dataframe\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72137d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the last five rows of dataframe\n",
    "tweet_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a94cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking random 5 rows of data\n",
    "tweet_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a concise summary of data\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "tweet_df.duplicated(subset='id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing duplicates\n",
    "pd.concat(g for _, g in tweet_df.groupby(\"id\") if len(g) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing data or null\n",
    "tweet_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ba4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking statistics of dataframe\n",
    "tweet_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatypes of each column\n",
    "tweet_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99db2b",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "The dataframe is cleaned in this section. Erroneous datatypes were changed to correct datatypes (id was converted from integer to string because it is a unique identifier and not to be used for calculations), Missing values were handled by filling with 'No Location', duplicate entries were dropped, the dataframe was reduced to the correct timeframe as well.\n",
    "\n",
    "Making a Copy of Data Before Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweet_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d71d1",
   "metadata": {},
   "source": [
    "## Issue 1:\n",
    "Define\n",
    "Convert time_of_tweet to datetime and tweet id to string.\n",
    "\n",
    "Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting to datetime using pandas to_datetime\n",
    "tweets_df['time_of_tweet'] = tweets_df['time_of_tweet'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc589fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting id to string\n",
    "tweets_df['id'] = tweets_df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f80eb4",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581766d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatypes\n",
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c1fa5",
   "metadata": {},
   "source": [
    "## Issue 2:\n",
    "Define\n",
    "Drop tweets before 2022-07-24 and after 2022-08-02\n",
    "\n",
    "Code\n",
    "\n",
    "Note, Twitter's time is UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39abbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying start date and end date\n",
    "start_date = '2022-07-24 00:00:00+00:00'\n",
    "end_date = '2022-08-03 00:00:00+00:00'\n",
    "\n",
    "# Dropping tweets\n",
    "mask = (tweets_df['time_of_tweet'] >= start_date) & (tweets_df['time_of_tweet'] < end_date)\n",
    "tweets_df = tweets_df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if the shape has been reduced since we dropped rows\n",
    "tweets_df.shape\n",
    "\n",
    "tweets_df.time_of_tweet.max()\n",
    "\n",
    "tweets_df.time_of_tweet.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b11e9e",
   "metadata": {},
   "source": [
    " COMMENT: Tweets are now ranging from 24th of July to 2nd of August. A span of 10 days\n",
    "\n",
    "## Issue 3:\n",
    "Define\n",
    "Drop Unnamed: 0 column from dataframe\n",
    "\n",
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnamed:0 column\n",
    "tweets_df.drop(columns = ['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13651a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "tweets_df = tweets_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if column has been dropped successfully\n",
    "tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600443ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b6f68",
   "metadata": {},
   "source": [
    "## Issue 4:\n",
    "Define\n",
    "Replace Missing values in Location with 'No Location'\n",
    "\n",
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad43e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling nulls in location column using fillna\n",
    "tweets_df.location.fillna('No Location', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ca89a",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values/ nulls\n",
    "tweets_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b27ec5",
   "metadata": {},
   "source": [
    "## Issue 5:\n",
    "Define\n",
    "Drop Duplicates\n",
    "\n",
    "Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b655d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates(subset='id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b7253",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd059fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "tweets_df.duplicated(subset='id').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6027b6",
   "metadata": {},
   "source": [
    "####  COMMENT: I'm done with all cleaning so I'll save it to a csv file.\n",
    "\n",
    "#### STORING CLEANED DATAFRAME TO A CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712eeeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('Renaissance_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b747079",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data preprocessing involves all data cleaning in preparing tweets for Sentiment analysis. To do this, I created several functions which I applied to 'tweet' column in my dataframe to produce desired results. Properly preprocessing data results in more accurate downstream processes.\n",
    "\n",
    "\n",
    "Also, for my Word Cloud, I wanted to show the words used to describe the album, so I created a function to extract some Positive Music words to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to extract hashtags with REGEX(Regular Expressions)\n",
    "def getHashtags(tweet):\n",
    "    tweet = tweet.lower()  #converts tweet to lower case\n",
    "    tweet = re.findall(r'\\#\\w+',tweet)  \n",
    "    return \" \".join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf8966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Hashtags and storing in column 'Hashtags'\n",
    "tweets_df['hashtags'] = tweets_df['tweet'].apply(getHashtags)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ccc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store hashtags in a list\n",
    "hashtags_list = tweets_df['hashtags'].tolist()\n",
    "\n",
    "# Iterate over all hashtags and split where there is more than one hashtag per row of data\n",
    "hashtags = []\n",
    "for item in hashtags_list:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        hashtags.append(i)\n",
    "\n",
    "# Importing Collection module to determine unique count of all hashtags used\n",
    "from collections import Counter\n",
    "\n",
    "# Determine Unique count of all hashtags used\n",
    "counts = Counter(hashtags)\n",
    "hashtags_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "hashtags_df.columns = ['hashtags', 'count']\n",
    "hashtags_df.sort_values(by='count', ascending=False, inplace=True)\n",
    "print(\"The Total Number of Unique Hashtags is: \", hashtags_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f01a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the hashtag dataframe for the top 10 hashtags used\n",
    "hashtags_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ae45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving hashtags dataframe to a csv file\n",
    "hashtags_df.to_csv('Ren_Hashtags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e56046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function\n",
    "def getTweetsLower(tweet):\n",
    "    tweet = tweet.lower()  #converts tweet to lower case\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc129e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tweets in lower case and store as tweet_lowercase\n",
    "tweets_df['tweet_lowercase'] = tweets_df['tweet'].apply(getTweetsLower)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66aa892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a List containing Renaissance Track\n",
    "renaissance_tracks = [\"i'mthatgirl\", \"cozy\", \"aliensuperstar\", \"cuffit\", \"energy\", \"breakmysoul\", \"churchgirl\", \n",
    "                     \"plasticoffthesofa\", \"virgo'sgroove\", \"move\", \"heated\", \"thique\", \"allupinyourmind\",\n",
    "                      \"americahasaproblem\", \"pure/honey\", \"summerrenaissance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b78602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to replace track names as one word track name in a new column tweet_track\n",
    "def trackNames(ren_track):\n",
    "    replacements = [(\"plastic off the sofa\",\"plasticoffthesofa\"), (\"i'm that girl\", \"i'mthatgirl\"), \n",
    "                    (\"im that girl\", \"i'mthatgirl\"),(\"alien superstar\", \"aliensuperstar\"), (\"cuff it\", \"cuffit\"), \n",
    "                    (\"break my soul\", \"breakmysoul\"), (\"church girl\", \"churchgirl\"), (\"virgo's groove\", \"virgo'sgroove\"), \n",
    "                    (\"virgo groove\", \"virgo'sgroove\"), (\"virgos groove\", \"virgo'sgroove\"), \n",
    "                    (\"all up in your mind\", \"allupinyourmind\"), (\"america has a problem\", \"americahasaproblem\"), \n",
    "                    (\"summer renaissance\", \"summerrenaissance\")]\n",
    "\n",
    "    for pat,repl in replacements:\n",
    "        ren_track = re.sub(pat, repl, ren_track)\n",
    "    return ren_track\n",
    "tweets_df['tweet_track'] = tweets_df['tweet_lowercase'].apply(trackNames)\n",
    "tweets_df.head()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91851e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract Renaissance Track from each tweet_track\n",
    "def getRenaissanceTrack(tweet_track):\n",
    "    tweet_track = tweet_track.lower() #Reduces tweet to lower case\n",
    "    tweet_tokens = word_tokenize(tweet_track) #splits each word in tweet_track for parsing\n",
    "    ren_track = [char for char in tweet_tokens if char in renaissance_tracks] \n",
    "    return \" \".join(ren_track)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dcfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tracks to a new column\n",
    "tweets_df['track'] = tweets_df['tweet_track'].apply(getRenaissanceTrack)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1efecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store track in a list\n",
    "track_list = tweets_df['track'].tolist()\n",
    "\n",
    "# Iterate over all track names and split where there is more than one track\n",
    "track = []\n",
    "for item in track_list:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        track.append(i)\n",
    "\n",
    "# Determine Unique count of all track\n",
    "counts = Counter(track)\n",
    "track_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "track_df.columns = ['Ren_track', 'Count']\n",
    "track_df.sort_values(by='Count', ascending=False, inplace=True)\n",
    "print(\"The Total Number of Unique Tracks is: \", track_df.shape[0])\n",
    "track_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Track df to csv file\n",
    "track_df.to_csv('Ren_Track.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b857829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of Positive words about the album\n",
    "renaissance_positive_words = [\"noskips\", \"noshuffles\", \"vocals\", \"lyrics\", \"beats\", \"production\", \n",
    "                     \"samples\", \"harmonies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e01ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to replace Positive words as one word Positive words in a new column tweet_positive_words\n",
    "def positiveWords(ren_positive_words):\n",
    "    replacements = [(\"no skips\",\"noskips\"), (\"zero skips\", \"noskips\"), (\"0 skips\", \"noskips\"), (\"no shuffle\", \"noshuffles\"), \n",
    "                    (\"no shuffles\", \"noshuffles\")]\n",
    "    for pat,repl in replacements:\n",
    "        ren_positive_words = re.sub(pat, repl, ren_positive_words)\n",
    "    return ren_positive_words\n",
    "tweets_df['tweet_positive_words'] = tweets_df['tweet_lowercase'].apply(positiveWords)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd18c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract Positive words from each Tweet\n",
    "def getPositiveWord(tweet_positive_words):\n",
    "    tweet_positive_words = tweet_positive_words.lower() #Reduces tweet to lower case\n",
    "    tweet_tokens = word_tokenize(tweet_positive_words) #splits each word in tweet_track for parsing\n",
    "    ren_positive_words = [char for char in tweet_tokens if char in renaissance_positive_words] \n",
    "    return \" \".join(ren_positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33832bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Positive words to a new column\n",
    "tweets_df['positive_words'] = tweets_df['tweet_positive_words'].apply(getPositiveWord)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store positive words in a list\n",
    "pos_list = tweets_df['positive_words'].tolist()\n",
    "\n",
    "# Iterate over all cast names and split where there is more than one cast\n",
    "pos = []\n",
    "for item in pos_list:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        pos.append(i)\n",
    "\n",
    "# Determine Unique count of all cast\n",
    "counts = Counter(pos)\n",
    "positive_words_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "positive_words_df.columns = ['Positive_Words', 'Count']\n",
    "positive_words_df.sort_values(by='Count', ascending=False, inplace=True)\n",
    "positive_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa096c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving positive words dataframe to a csv file\n",
    "positive_words_df.to_csv('Ren_Positive_Words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Defining my NLTK stop words and my user-defined stop words\n",
    "stop_words = list(stopwords.words('english'))\n",
    "user_stop_words = [\"i\", \"i'm\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "                   \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "                   \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "                   \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
    "                   \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"i'll\",\"we'll\",\"they'll\",\"you'll\",\"she'll\",\"he'll\",\"'ll\",\"n't\",\n",
    "                   \"'s\",'anyone','today','yesterday','day', 'already', 'year', 'many', 'much', 'amp', 'next', 'cant', 'wont', 'hadnt','havent', 'hasnt', \n",
    "                   'isnt', 'shouldnt', \"didn't\", \"couldn't\", 'wasnt', 'werent','mustnt', \n",
    "                   'been…','aht', 've', 'next',\"all\", \"any\", \"both\", \"each\", 'by',\n",
    "                  'year',]\n",
    "\n",
    "# The list below are common words which will not be relevant in our analysis.\n",
    "common_words = ['renaissance', 'beyonce', 'bey', 'rennaissance', 'album', \"beyonce's\", \"beehive\", \"transitions\"]\n",
    "alphabets = list(string.ascii_lowercase)\n",
    "stop_words = stop_words + user_stop_words + alphabets + common_words + renaissance_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = list(UNICODE_EMOJI.keys())\n",
    "\n",
    "# preProcess tweet for sentiment analysis\n",
    "def preprocessTweets(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    # Cleaning and removing URL’s\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags = re.MULTILINE)\n",
    "    # Cleaning and removing repeating characters\n",
    "    tweet = re.sub(r'\\@\\w+|\\#\\w+|\\d+', '',  tweet)\n",
    "    # Cleaning and removing the above stop words list from the tweet text\n",
    "    tweet_tokens = word_tokenize(tweet)  \n",
    "    filtered_words = [w for w in tweet_tokens if w not in stop_words]\n",
    "    filtered_words = [w for w in filtered_words if w not in emojis]\n",
    "    # Cleaning and removing punctuations\n",
    "    unpunctuated_words = [w for w in filtered_words if w not in string.punctuation]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in unpunctuated_words]\n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new column called 'Processed Tweets' by applying preprocessed tweets function to the 'Tweet' column.\n",
    "tweets_df['Processed_Tweets'] = tweets_df['tweet'].apply(preprocessTweets)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tweets into one long string with each word separate with a \"space\"\n",
    "tweets_long_string = tweets_df['Processed_Tweets'].tolist()\n",
    "tweets_long_string = \" \".join(tweets_long_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9152b85c",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "In this section, I want to show the sentiments in relation to the Renaissance Album. I'm going to employ the use of Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ea93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2073407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to obtain Polarity Score\n",
    "def getPolarity(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "# Define function to obtain Sentiment category\n",
    "def getSentimentTextBlob(polarity):\n",
    "    if polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3717ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions to respective columns\n",
    "tweets_df['Polarity']=tweets_df['Processed_Tweets'].apply(getPolarity)\n",
    "tweets_df['Sentiment']=tweets_df['Polarity'].apply(getSentim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values and count in sentiment column\n",
    "tweets_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c088310",
   "metadata": {},
   "source": [
    "## Data Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for opening, manipulating, and saving image file\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tweets_df['Sentiment'].value_counts()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie chart to show distribution of Sentiments\n",
    "plt.figure(figsize=[15,10], facecolor='none')\n",
    "plt.pie(data, labels=['Positive', 'Negative'], colors=['#FEE715', '#666666'], startangle=90, explode= [0.05, 0.05], autopct='%1.1f%%');\n",
    "plt.title('Twitter Users Sentiments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to dataframe\n",
    "plt.savefig(\"Sentiments.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f9572",
   "metadata": {},
   "source": [
    "#### Next, Plotting Tracks to show rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df_sort = track_df.sort_values('Count', ascending='True')\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "ax = track_df_sort.plot.barh(x='Ren_track', y='Count', figsize=(10,15), legend=None, width=0.6, color=['#666666', '#666666','#666666','#666666','#666666','#666666','#666666','#666666','#666666','#666666','#666666','#666666','#666666','#666666', '#666666', '#FEE715'])\n",
    "plt.title('Ranking Of Renaissance Tracks');\n",
    "plt.grid(False)\n",
    "plt.ylabel('Tracks')\n",
    "plt.xlabel('Count')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "\n",
    "for y, x in enumerate(track_df_sort.Count):\n",
    "    ax.annotate(\"{:,}\".format(x), xy=(x, y))\n",
    "    ax.set_xlim(0, track_df_sort.Count.max()*1.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26800f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to dataframe\n",
    "plt.savefig(\"Tracks.png\", format=\"png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747987e",
   "metadata": {},
   "source": [
    "#### Next, Generating WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to generate the blue colour for the Word CLoud\n",
    "def yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n",
    "    return \"hsl(54, 99%%, %d%%)\" % random.randint(50, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Import Horse Logo\n",
    "image = np.array(Image.open('RenaissanceHorse.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Instantiate the figure object\n",
    "plt.figure(figsize=[15,10], facecolor='none')\n",
    "\n",
    "\n",
    "plt.imshow(image, cmap=plt.cm.gray, interpolation='bilinear') # Display data as an image\n",
    "plt.axis('off') # Remove axis\n",
    "plt.show() # Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Twitter word cloud object\n",
    "twitter_wc = WordCloud(background_color='#212121', mask=image)\n",
    "\n",
    "# generate the word cloud\n",
    "twitter_wc.generate(tweets_long_string)\n",
    "\n",
    "# display the word cloud\n",
    "plt.figure(figsize=[15,10], facecolor='none')\n",
    "\n",
    "\n",
    "plt.imshow(twitter_wc.recolor(color_func = yellow_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud Representation of Tweets');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to dataframe\n",
    "plt.savefig(\"RenaissanceWordCloud.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10961c5b",
   "metadata": {},
   "source": [
    "### Save the Dataframe to be exported to Microsoft Power BI to Create a Dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(\"Renaissance_Final_File.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a7ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18d7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
